{'2aff33a3ef12402bb61a2c6deb4dbca9__import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nimport io\n\ndef get_wikitext2_data():\n    """\n    获取并返回 WikiText-2 数据集的训练、验证和测试部分。\n    """\n    # 使用 trust_remote_code=True 来避免一个潜在的脚本加载问题\n    wikitext2 = load_dataset("wikitext", "wikitext-2-raw-v1")\n    return wikitext2["train"], wikitext2["validation"], wikitext2["test"]\n\ndef build_vocab(text_data):\n    """\n    根据文本数据构建词汇表。\n    """\n    counter = Counter()\n    for line in text_data:\n        if line.strip(): # 确保行不为空\n            counter.update(line.strip().split())\n    # 创建词汇表时，添加特殊标记\n    vocab = Vocab(counter, specials=["<unk>", "<pad>", "<bos>", "<eos>"])\n    vocab.set_default_index(vocab["<unk>"])\n    return vocab\n\nclass LanguageModelDataset(Dataset):\n    """\n    自定义语言模型数据集。\n    """\n    def __init__(self, data, vocab, bptt):\n        self.vocab = vocab\n        self.bptt = bptt # bptt: backpropagation through time, 即序列长度\n        self.data = self.process_data(data)\n\n    def process_data(self, raw_data):\n        """\n        将原始文本数据转换为一个长序列的 token IDs。\n        """\n        token_ids = []\n        for line in raw_data:\n            if line.strip():\n                tokens = ["<bos>"] + line.strip().split() + ["<eos>"]\n                token_ids.extend([self.vocab[token] for token in tokens])\n        return torch.tensor(token_ids, dtype=torch.long)\n\n    def __len__(self):\n        # 总长度除以序列长度，得到样本数量\n        return (len(self.data) - 1) // self.bptt\n\n    def __getitem__(self, idx):\n        """\n        获取一个样本。源数据是目标数据向右移动一位。\n        """\n        start_idx = idx * self.bptt\n        end_idx = start_idx + self.bptt\n        # 源序列\n        src = self.data[start_idx:end_idx]\n        # 目标序列是源序列的下一个词\n        tgt = self.data[start_idx+1:end_idx+1]\n        return src, tgt\n\ndef get_data_loaders(batch_size, bptt=35):\n    """\n    主函数，用于获取数据加载器。\n    """\n    print("正在加载和预处理数据...")\n    train_raw, valid_raw, test_raw = get_wikitext2_data()\n    \n    # 从原始数据中提取文本内容\n    train_text = [item["text"] for item in train_raw if item["text"].strip()]\n    valid_text = [item["text"] for item in valid_raw if item["text"].strip()]\n    test_text = [item["text"] for item in test_raw if item["text"].strip()]\n\n    # 构建词汇表\n    vocab = build_vocab(train_text)\n    print(f"词汇表大小: {len(vocab)}")\n\n    # 创建数据集\n    train_dataset = LanguageModelDataset(train_text, vocab, bptt)\n    valid_dataset = LanguageModelDataset(valid_text, vocab, bptt)\n    test_dataset = LanguageModelDataset(test_text, vocab, bptt)\n\n    # 创建数据加载器\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    print("数据加载器创建完成。")\n    return train_loader, valid_loader, test_loader, vocab\n\nif __name__ == "__main__":\n    # 这是一个简单的测试，以确保数据加载器能够正常工作\n    BATCH_SIZE = 32\n    BPTT = 35 # 序列长度\n    train_loader, _, _, vocab = get_data_loaders(BATCH_SIZE, BPTT)\n    \n    print("\\n--- 测试数据加载器 ---")\n    print(f"词汇表大小: {len(vocab)}")\n    \n    # 从训练加载器中获取一个批次的数据\n    for i, (src, tgt) in enumerate(train_loader):\n        print(f"批次 {i+1}:")\n        print(f"  源数据形状 (src): {src.shape}")\n        print(f"  目标数据形状 (tgt): {tgt.shape}")\n        \n        # 验证 src 和 tgt 的形状是否匹配\n        assert src.shape == (BATCH_SIZE, BPTT), "源数据形状不正确"\n        assert tgt.shape == (BATCH_SIZE, BPTT), "目标数据形状不正确"\n        \n        # 打印第一个样本的部分内容\n        print("\\n  第一个样本示例:")\n        print(f"    源 (tokens): {\' \'.join([vocab.itos[token_id] for token_id in src[0, :10]])}")\n        print(f"    目标 (tokens): {\' \'.join([vocab.itos[token_id] for token_id in tgt[0, :10]])}")\n        \n        if i == 0: # 只测试一个批次\n            break\n            \n    print("\\n--- 数据加载器测试成功 ---")\n': ['src/dataset.py', '<Non-compressible placeholder 1>'], '73ed0f7caeb2450287e2f71f6ad83ebf__import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nimport zipfile\nimport io\nimport os\nfrom collections import Counter\n\nclass Vocabulary:\n    """词汇表类，用于管理词到索引的映射"""\n    def __init__(self, counter, specials):\n        self.specials = specials\n        self.itos = list(specials)\n        # 按频率降序排序\n        for word, _ in counter.most_common():\n            if word not in self.itos:\n                self.itos.append(word)\n        \n        self.stoi = {word: i for i, word in enumerate(self.itos)}\n        self.unk_index = self.stoi["<unk>"]\n\n    def __len__(self):\n        return len(self.itos)\n\n    def get_index(self, word):\n        return self.stoi.get(word, self.unk_index)\n\n    def get_word(self, index):\n        return self.itos[index] if 0 <= index < len(self.itos) else "<unk>"\n\ndef download_and_extract_wikitext2(data_path="."):\n    """\n    下载并解压 WikiText-2 数据集。\n    """\n    url = "https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip"\n    zip_path = os.path.join(data_path, "wikitext-2-v1.zip")\n    extracted_path = os.path.join(data_path, "wikitext-2")\n\n    if os.path.exists(extracted_path):\n        print("数据集已存在，跳过下载和解压。")\n        return extracted_path\n\n    print("正在下载 WikiText-2 数据集...")\n    try:\n        r = requests.get(url)\n        r.raise_for_status() # 如果下载失败则抛出异常\n        z = zipfile.ZipFile(io.BytesIO(r.content))\n        print("正在解压数据集...")\n        z.extractall(data_path)\n        print(f"数据集已成功解压到: {extracted_path}")\n        return extracted_path\n    except requests.exceptions.RequestException as e:\n        print(f"下载失败: {e}")\n        return None\n\ndef build_vocab_from_file(filepath):\n    """\n    从文件构建词汇表。\n    """\n    counter = Counter()\n    with open(filepath, "r", encoding="utf8") as f:\n        for line in f:\n            # 过滤掉空行和标题行\n            if line.strip() and not line.strip().startswith("="):\n                counter.update(line.strip().split())\n    \n    vocab = Vocabulary(counter, specials=["<unk>", "<pad>", "<bos>", "<eos>"])\n    return vocab\n\nclass LanguageModelDataset(Dataset):\n    """\n    自定义语言模型数据集。\n    """\n    def __init__(self, data_tensor, bptt):\n        self.data = data_tensor\n        self.bptt = bptt\n\n    def __len__(self):\n        # 确保我们不会因为目标序列而出界\n        return (len(self.data) - 1) // self.bptt\n\n    def __getitem__(self, idx):\n        start_idx = idx * self.bptt\n        end_idx = start_idx + self.bptt\n        src = self.data[start_idx:end_idx]\n        # 目标是源序列向右移动一位\n        tgt = self.data[start_idx + 1 : end_idx + 1]\n        return src, tgt\n\ndef tokenize_file(filepath, vocab):\n    """\n    将文件中的文本转换为 token ID 的长序列。\n    """\n    token_ids = []\n    with open(filepath, "r", encoding="utf8") as f:\n        for line in f:\n            if line.strip() and not line.strip().startswith("="):\n                tokens = ["<bos>"] + line.strip().split() + ["<eos>"]\n                token_ids.extend([vocab.get_index(token) for token in tokens])\n    return torch.tensor(token_ids, dtype=torch.long)\n\n\ndef get_data_loaders(batch_size, bptt=35):\n    """\n    主函数，用于获取数据加载器。\n    """\n    print("--- 开始数据处理 ---")\n    data_dir = download_and_extract_wikitext2()\n    if data_dir is None:\n        raise RuntimeError("无法下载或找到数据集。")\n\n    train_path = os.path.join(data_dir, "wiki.train.tokens")\n    valid_path = os.path.join(data_dir, "wiki.valid.tokens")\n    test_path = os.path.join(data_dir, "wiki.test.tokens")\n\n    # 1. 构建词汇表\n    print("正在从训练数据构建词汇表...")\n    vocab = build_vocab_from_file(train_path)\n    print(f"词汇表大小: {len(vocab)}")\n\n    # 2. 将文本文件转换为 Tensors\n    print("正在处理训练、验证和测试数据...")\n    train_data = tokenize_file(train_path, vocab)\n    valid_data = tokenize_file(valid_path, vocab)\n    test_data = tokenize_file(test_path, vocab)\n\n    # 3. 创建数据集\n    train_dataset = LanguageModelDataset(train_data, bptt)\n    valid_dataset = LanguageModelDataset(valid_data, bptt)\n    test_dataset = LanguageModelDataset(test_data, bptt)\n\n    # 4. 创建数据加载器\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    print("--- 数据加载器创建完成 ---")\n    return train_loader, valid_loader, test_loader, vocab\n\nif __name__ == "__main__":\n    BATCH_SIZE = 32\n    BPTT = 35\n    \n    try:\n        train_loader, valid_loader, test_loader, vocab = get_data_loaders(BATCH_SIZE, BPTT)\n        \n        print("\\n--- 测试数据加载器 ---")\n        print(f"词汇表大小: {len(vocab)}")\n        \n        # 从训练加载器中获取一个批次的数据\n        src, tgt = next(iter(train_loader))\n        \n        print(f"\\n单个批次的数据形状:")\n        print(f"  源数据形状 (src): {src.shape}")\n        print(f"  目标数据形状 (tgt): {tgt.shape}")\n        \n        assert src.shape == (BATCH_SIZE, BPTT), "源数据形状不正确"\n        assert tgt.shape == (BATCH_SIZE, BPTT), "目标数据形状不正确"\n        \n        print("\\n第一个样本示例:")\n        print(f"  源 (tokens): {\' \'.join([vocab.get_word(token_id) for token_id in src[0, :15]])}")\n        print(f"  目标 (tokens): {\' \'.join([vocab.get_word(token_id) for token_id in tgt[0, :15]])}")\n        \n        print("\\n--- 数据加载器测试成功 ---")\n\n    except Exception as e:\n        print(f"\\n--- 测试失败 ---")\n        print(f"错误信息: {e}")\n': ['src/dataset.py', '<Non-compressible placeholder 2>'], '35c13b42532e48769982197c23d6b0fe__import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport requests\nimport zipfile\nimport io\nimport os\nfrom collections import Counter\n\nclass Vocabulary:\n    """词汇表类，用于管理词到索引的映射"""\n    def __init__(self, counter, specials):\n        self.specials = specials\n        self.itos = list(specials)\n        # 按频率降序排序\n        for word, _ in counter.most_common():\n            if word not in self.itos:\n                self.itos.append(word)\n        \n        self.stoi = {word: i for i, word in enumerate(self.itos)}\n        self.unk_index = self.stoi["<unk>"]\n\n    def __len__(self):\n        return len(self.itos)\n\n    def get_index(self, word):\n        return self.stoi.get(word, self.unk_index)\n\n    def get_word(self, index):\n        return self.itos[index] if 0 <= index < len(self.itos) else "<unk>"\n\ndef download_and_extract_wikitext2(data_path="."):\n    """\n    下载并解压 WikiText-2 数据集。\n    """\n    # 更新为新的下载链接\n    url = "https://github.com/LogSSim/deeplearning_d2l_classes/raw/main/class14_BERT/wikitext-2-v1.zip"\n    zip_path = os.path.join(data_path, "wikitext-2-v1.zip")\n    extracted_path = os.path.join(data_path, "wikitext-2")\n\n    if os.path.exists(extracted_path):\n        print("数据集已存在，跳过下载和解压。")\n        return extracted_path\n\n    print("正在下载 WikiText-2 数据集...")\n    try:\n        r = requests.get(url)\n        r.raise_for_status() # 如果下载失败则抛出异常\n        z = zipfile.ZipFile(io.BytesIO(r.content))\n        print("正在解压数据集...")\n        z.extractall(data_path)\n        print(f"数据集已成功解压到: {extracted_path}")\n        return extracted_path\n    except requests.exceptions.RequestException as e:\n        print(f"下载失败: {e}")\n        return None\n    except zipfile.BadZipFile as e:\n        print(f"解压失败: {e}. 下载的文件可能不是一个有效的zip文件。")\n        return None\n\ndef build_vocab_from_file(filepath):\n    """\n    从文件构建词汇表。\n    """\n    counter = Counter()\n    with open(filepath, "r", encoding="utf8") as f:\n        for line in f:\n            # 过滤掉空行和标题行\n            if line.strip() and not line.strip().startswith("="):\n                counter.update(line.strip().split())\n    \n    vocab = Vocabulary(counter, specials=["<unk>", "<pad>", "<bos>", "<eos>"])\n    return vocab\n\nclass LanguageModelDataset(Dataset):\n    """\n    自定义语言模型数据集。\n    """\n    def __init__(self, data_tensor, bptt):\n        self.data = data_tensor\n        self.bptt = bptt\n\n    def __len__(self):\n        # 确保我们不会因为目标序列而出界\n        return (len(self.data) - 1) // self.bptt\n\n    def __getitem__(self, idx):\n        start_idx = idx * self.bptt\n        end_idx = start_idx + self.bptt\n        src = self.data[start_idx:end_idx]\n        # 目标是源序列向右移动一位\n        tgt = self.data[start_idx + 1 : end_idx + 1]\n        return src, tgt\n\ndef tokenize_file(filepath, vocab):\n    """\n    将文件中的文本转换为 token ID 的长序列。\n    """\n    token_ids = []\n    with open(filepath, "r", encoding="utf8") as f:\n        for line in f:\n            if line.strip() and not line.strip().startswith("="):\n                tokens = ["<bos>"] + line.strip().split() + ["<eos>"]\n                token_ids.extend([vocab.get_index(token) for token in tokens])\n    return torch.tensor(token_ids, dtype=torch.long)\n\n\ndef get_data_loaders(batch_size, bptt=35):\n    """\n    主函数，用于获取数据加载器。\n    """\n    print("--- 开始数据处理 ---")\n    data_dir = download_and_extract_wikitext2()\n    if data_dir is None:\n        raise RuntimeError("无法下载或找到数据集。")\n\n    train_path = os.path.join(data_dir, "wikitext-2", "wiki.train.tokens")\n    valid_path = os.path.join(data_dir, "wikitext-2", "wiki.valid.tokens")\n    test_path = os.path.join(data_dir, "wikitext-2", "wiki.test.tokens")\n\n    # 1. 构建词汇表\n    print("正在从训练数据构建词汇表...")\n    vocab = build_vocab_from_file(train_path)\n    print(f"词汇表大小: {len(vocab)}")\n\n    # 2. 将文本文件转换为 Tensors\n    print("正在处理训练、验证和测试数据...")\n    train_data = tokenize_file(train_path, vocab)\n    valid_data = tokenize_file(valid_path, vocab)\n    test_data = tokenize_file(test_path, vocab)\n\n    # 3. 创建数据集\n    train_dataset = LanguageModelDataset(train_data, bptt)\n    valid_dataset = LanguageModelDataset(valid_data, bptt)\n    test_dataset = LanguageModelDataset(test_data, bptt)\n\n    # 4. 创建数据加载器\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n    \n    print("--- 数据加载器创建完成 ---")\n    return train_loader, valid_loader, test_loader, vocab\n\nif __name__ == "__main__":\n    BATCH_SIZE = 32\n    BPTT = 35\n    \n    try:\n        train_loader, valid_loader, test_loader, vocab = get_data_loaders(BATCH_SIZE, BPTT)\n        \n        print("\\n--- 测试数据加载器 ---")\n        print(f"词汇表大小: {len(vocab)}")\n        \n        # 从训练加载器中获取一个批次的数据\n        src, tgt = next(iter(train_loader))\n        \n        print(f"\\n单个批次的数据形状:")\n        print(f"  源数据形状 (src): {src.shape}")\n        print(f"  目标数据形状 (tgt): {tgt.shape}")\n        \n        assert src.shape == (BATCH_SIZE, BPTT), "源数据形状不正确"\n        assert tgt.shape == (BATCH_SIZE, BPTT), "目标数据形状不正确"\n        \n        print("\\n第一个样本示例:")\n        print(f"  源 (tokens): {\\\' \\\'.join([vocab.get_word(token_id) for token_id in src[0, :15]])}")\n        print(f"  目标 (tokens): {\\\' \\\'.join([vocab.get_word(token_id) for token_id in tgt[0, :15]])}")\n        \n        print("\\n--- 数据加载器测试成功 ---")\n\n    except Exception as e:\n        print(f"\\n--- 测试失败 ---")\n        print(f"错误信息: {e}")\n': ['src/dataset.py', '<Non-compressible placeholder 3>'], '2480ec04a0674493a02d342ac874f588__In the file `src/dataset.py`, find the line `print(f"  源 (tokens): {\\\' \\\'.join([vocab.get_word(token_id) for token_id in src[0, :15]])}")` and replace it with `print(f"  源 (tokens): {\' \'.join([vocab.get_word(token_id) for token_id in src[0, :15]])}")`. Also, find the line `print(f"  目标 (tokens): {\\\' \\\'.join([vocab.get_word(token_id) for token_id in tgt[0, :15]])}")` and replace it with `print(f"  目标 (tokens): {\' \'.join([vocab.get_word(token_id) for token_id in tgt[0, :15]])}")`.': ['src/dataset.py', '<Non-compressible placeholder 4>'], '1918c4ebf2d14482af104126a36eff60__In the file `src/dataset.py`, find and replace the following lines:\n`train_path = os.path.join(data_dir, "wikitext-2", "wiki.train.tokens")` with `train_path = os.path.join(data_dir, "wiki.train.tokens")`\n`valid_path = os.path.join(data_dir, "wikitext-2", "wiki.valid.tokens")` with `valid_path = os.path.join(data_dir, "wiki.valid.tokens")`\n`test_path = os.path.join(data_dir, "wikitext-2", "wiki.test.tokens")` with `test_path = os.path.join(data_dir, "wiki.test.tokens")`': ['src/dataset.py', '<Non-compressible placeholder 5>'], '4b402eb7d10f4af39ca90d931c72d41c__import torch\nimport torch.nn as nn\nimport math\n\nclass PositionalEncoding(nn.Module):\n    """\n    实现位置编码模块。\n    """\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer("pe", pe)\n\n    def forward(self, x):\n        # x 的形状: [seq_len, batch_size, d_model]\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass MultiHeadAttention(nn.Module):\n    """\n    实现多头自注意力机制。\n    """\n    def __init__(self, d_model, n_heads, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % n_heads == 0, "d_model 必须能被 n_heads 整除"\n\n        self.d_k = d_model // n_heads\n        self.n_heads = n_heads\n        \n        self.q_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        \n        self.out = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def scaled_dot_product_attention(self, q, k, v, mask=None):\n        # q, k, v 的形状: [batch_size, n_heads, seq_len, d_k]\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n            \n        attention_weights = torch.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        output = torch.matmul(attention_weights, v)\n        return output, attention_weights\n\n    def forward(self, x, mask=None):\n        # x 的形状: [batch_size, seq_len, d_model]\n        batch_size = x.size(0)\n\n        # 1. 线性变换并切分头\n        q = self.q_linear(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        k = self.k_linear(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n        v = self.v_linear(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n        # 2. 计算缩放点积注意力\n        # attention_output 形状: [batch_size, n_heads, seq_len, d_k]\n        attention_output, _ = self.scaled_dot_product_attention(q, k, v, mask)\n\n        # 3. 合并头\n        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n        \n        # 4. 最终线性变换\n        output = self.out(attention_output)\n        return output\n\nclass PositionwiseFeedForward(nn.Module):\n    """\n    实现逐位置前馈网络。\n    """\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # x 的形状: [batch_size, seq_len, d_model]\n        x = self.relu(self.linear1(x))\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\nclass EncoderBlock(nn.Module):\n    """\n    实现一个 Transformer Encoder Block。\n    """\n    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n        super(EncoderBlock, self).__init__()\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # x 的形状: [batch_size, seq_len, d_model]\n        \n        # 1. 多头注意力 + Add & Norm\n        attn_output = self.attention(x, mask)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n        \n        # 2. 前馈网络 + Add & Norm\n        ffn_output = self.ffn(x)\n        x = x + self.dropout2(ffn_output)\n        x = self.norm2(x)\n        \n        return x\n\nclass TransformerEncoder(nn.Module):\n    """\n    实现完整的 Transformer Encoder 模型。\n    """\n    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, dropout=0.1):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        self.layers = nn.ModuleList([\n            EncoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n        ])\n        \n        self.output_layer = nn.Linear(d_model, vocab_size)\n        self.d_model = d_model\n\n    def forward(self, src, src_mask=None):\n        # src 的形状: [batch_size, seq_len]\n        \n        # 1. 嵌入和位置编码\n        # embed_src 形状: [batch_size, seq_len, d_model]\n        embed_src = self.embedding(src) * math.sqrt(self.d_model)\n        \n        # PyTorch 的 Transformer 模块期望输入形状为 [seq_len, batch_size, d_model]\n        # 我们将保持 [batch_size, seq_len, d_model] 并在需要时调整\n        \n        # 调整为 [seq_len, batch_size, d_model] 以适应 PositionalEncoding\n        pos_encoded_src = self.pos_encoder(embed_src.transpose(0, 1)).transpose(0, 1)\n        \n        # 2. 通过 Encoder Blocks\n        encoder_output = pos_encoded_src\n        for layer in self.layers:\n            encoder_output = layer(encoder_output, src_mask)\n            \n        # 3. 输出层\n        output = self.output_layer(encoder_output)\n        return output\n\nif __name__ == "__main__":\n    # --- 模型组件测试 ---\n    print("--- 开始模型组件测试 ---")\n    \n    # 超参数\n    VOCAB_SIZE = 10000\n    D_MODEL = 128\n    N_HEADS = 4\n    D_FF = 512\n    N_LAYERS = 2\n    DROPOUT = 0.1\n    BATCH_SIZE = 32\n    SEQ_LEN = 35\n\n    # 创建一个假的输入张量\n    test_input = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\n    \n    # 1. 测试 PositionalEncoding\n    print("\\n1. 测试 PositionalEncoding...")\n    embedding = nn.Embedding(VOCAB_SIZE, D_MODEL)\n    pos_encoder = PositionalEncoding(D_MODEL, DROPOUT)\n    embedded_input = embedding(test_input).transpose(0, 1) # -> [seq_len, batch_size, d_model]\n    pos_encoded_output = pos_encoder(embedded_input)\n    print(f"  PositionalEncoding 输入形状: {embedded_input.shape}")\n    print(f"  PositionalEncoding 输出形状: {pos_encoded_output.shape}")\n    assert pos_encoded_output.shape == (SEQ_LEN, BATCH_SIZE, D_MODEL)\n    print("  PositionalEncoding 测试通过。")\n\n    # 2. 测试 MultiHeadAttention\n    print("\\n2. 测试 MultiHeadAttention...")\n    multi_head_attn = MultiHeadAttention(D_MODEL, N_HEADS, DROPOUT)\n    # 输入形状: [batch_size, seq_len, d_model]\n    attn_input = pos_encoded_output.transpose(0, 1) \n    attn_output = multi_head_attn(attn_input)\n    print(f"  MultiHeadAttention 输入形状: {attn_input.shape}")\n    print(f"  MultiHeadAttention 输出形状: {attn_output.shape}")\n    assert attn_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)\n    print("  MultiHeadAttention 测试通过。")\n\n    # 3. 测试 PositionwiseFeedForward\n    print("\\n3. 测试 PositionwiseFeedForward...")\n    ffn = PositionwiseFeedForward(D_MODEL, D_FF, DROPOUT)\n    ffn_output = ffn(attn_output)\n    print(f"  PositionwiseFeedForward 输入形状: {attn_output.shape}")\n    print(f"  PositionwiseFeedForward 输出形状: {ffn_output.shape}")\n    assert ffn_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)\n    print("  PositionwiseFeedForward 测试通过。")\n\n    # 4. 测试 EncoderBlock\n    print("\\n4. 测试 EncoderBlock...")\n    encoder_block = EncoderBlock(D_MODEL, N_HEADS, D_FF, DROPOUT)\n    encoder_block_output = encoder_block(attn_input)\n    print(f"  EncoderBlock 输入形状: {attn_input.shape}")\n    print(f"  EncoderBlock 输出形状: {encoder_block_output.shape}")\n    assert encoder_block_output.shape == (BATCH_SIZE, SEQ_LEN, D_MODEL)\n    print("  EncoderBlock 测试通过。")\n\n    # 5. 测试完整的 TransformerEncoder\n    print("\\n5. 测试完整的 TransformerEncoder...")\n    model = TransformerEncoder(VOCAB_SIZE, D_MODEL, N_HEADS, D_FF, N_LAYERS, DROPOUT)\n    model_output = model(test_input)\n    print(f"  TransformerEncoder 输入形状: {test_input.shape}")\n    print(f"  TransformerEncoder 输出形状: {model_output.shape}")\n    assert model_output.shape == (BATCH_SIZE, SEQ_LEN, VOCAB_SIZE)\n    print("  TransformerEncoder 测试通过。")\n\n    print("\\n--- 所有模型组件测试成功 ---")\n': ['src/model.py', '<Non-compressible placeholder 6>'], '4dad5911990a4b0b95d301b86f86a41c__import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nimport math\nimport time\nimport os\nimport json\nimport matplotlib.pyplot as plt\n\n# 导入我们自己编写的模块\nfrom src.dataset import get_data_loaders\nfrom src.model import TransformerEncoder\n\ndef train_one_epoch(model, data_loader, optimizer, criterion, device):\n    """\n    训练一个 epoch。\n    """\n    model.train() # 设置为训练模式\n    total_loss = 0.\n    start_time = time.time()\n\n    for i, (src, tgt) in enumerate(data_loader):\n        src, tgt = src.to(device), tgt.to(device)\n        \n        optimizer.zero_grad()\n        \n        # 模型输出形状: [batch_size, seq_len, vocab_size]\n        output = model(src)\n        \n        # 为了计算损失，需要将 output 和 tgt 变形\n        # output -> [batch_size * seq_len, vocab_size]\n        # tgt -> [batch_size * seq_len]\n        loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # 梯度裁剪\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n        if i % 200 == 0 and i > 0:\n            cur_loss = total_loss / (i + 1)\n            elapsed = time.time() - start_time\n            print(f"| epoch {epoch:3d} | {i:5d}/{len(data_loader):5d} batches | "\n                  f"loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}")\n            \n    return total_loss / len(data_loader)\n\ndef evaluate(model, data_loader, criterion, device):\n    """\n    在验证集或测试集上评估模型。\n    """\n    model.eval() # 设置为评估模式\n    total_loss = 0.\n    \n    with torch.no_grad():\n        for src, tgt in data_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            output = model(src)\n            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n            total_loss += loss.item()\n            \n    return total_loss / len(data_loader)\n\ndef main():\n    """\n    主函数，执行完整的训练和评估流程。\n    """\n    # --- 超参数设置 (后续将从 configs/base.yaml 加载) ---\n    # 模型参数\n    EMBED_DIM = 128\n    N_HEADS = 4\n    FFN_DIM = 512\n    N_LAYERS = 2\n    DROPOUT = 0.1\n    \n    # 训练参数\n    BATCH_SIZE = 32\n    LEARNING_RATE = 3e-4\n    EPOCHS = 5 # 为了快速演示，只训练5个epoch\n    BPTT = 35 # 序列长度\n\n    # --- 环境设置 ---\n    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n    print(f"使用设备: {device}")\n\n    # --- 数据加载 ---\n    # 注意：get_data_loaders 现在返回 vocab 对象\n    train_loader, valid_loader, test_loader, vocab = get_data_loaders(BATCH_SIZE, BPTT)\n    vocab_size = len(vocab)\n    print(f"词汇表大小: {vocab_size}")\n\n    # --- 模型、损失函数、优化器初始化 ---\n    model = TransformerEncoder(\n        vocab_size=vocab_size,\n        d_model=EMBED_DIM,\n        n_heads=N_HEADS,\n        d_ff=FFN_DIM,\n        n_layers=N_LAYERS,\n        dropout=DROPOUT\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # --- 训练循环 ---\n    train_losses = []\n    valid_losses = []\n\n    for epoch in range(1, EPOCHS + 1):\n        epoch_start_time = time.time()\n        \n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n        valid_loss = evaluate(model, valid_loader, criterion, device)\n        \n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        \n        elapsed = time.time() - epoch_start_time\n        print("-" * 89)\n        print(f"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | "\n              f"valid loss {valid_loss:5.2f} | valid ppl {math.exp(valid_loss):8.2f}")\n        print("-" * 89)\n\n    # --- 结果保存 ---\n    # 确保 results 目录存在\n    os.makedirs("results", exist_ok=True)\n\n    # 1. 保存训练曲线图\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, EPOCHS + 1), train_losses, label="Training Loss")\n    plt.plot(range(1, EPOCHS + 1), valid_losses, label="Validation Loss")\n    plt.xlabel("Epoch")\n    plt.ylabel("Loss")\n    plt.title("Training and Validation Loss Curves")\n    plt.legend()\n    plt.grid(True)\n    plot_path = "results/training_curves.png"\n    plt.savefig(plot_path)\n    print(f"训练曲线图已保存到: {plot_path}")\n\n    # 2. 保存最终评估指标\n    final_metrics = {\n        "final_validation_loss": valid_losses[-1],\n        "final_validation_perplexity": math.exp(valid_losses[-1])\n    }\n    metrics_path = "results/metrics.json"\n    with open(metrics_path, "w") as f:\n        json.dump(final_metrics, f, indent=4)\n    print(f"最终评估指标已保存到: {metrics_path}")\n\nif __name__ == "__main__":\n    main()\n': ['src/train.py', '<Non-compressible placeholder 7>'], 'e67036e7a2aa40af9a5273a7834d2243__In the file `src/train.py`:\n1. Find the line `def train_one_epoch(model, data_loader, optimizer, criterion, device):` and replace it with `def train_one_epoch(model, data_loader, optimizer, criterion, device, epoch):`\n2. Find the line `train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)` and replace it with `train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)`': ['src/train.py', '<Non-compressible placeholder 8>'], 'fdf28c995fa14f06a8a4c0472dce0525__# 一键运行训练的 shell 脚本\npython src/train.py\n': ['scripts/run.sh', '<Non-compressible placeholder 9>'], '1f6aaf5e759b447f83c9868b08bb2265__model:\n  embed_dim: 128\n  n_heads: 4\n  ffn_dim: 512\n  n_layers: 2\n  dropout: 0.1\ntraining:\n  batch_size: 32\n  learning_rate: 3e-4\n  epochs: 5\n  bptt: 35\n  seed: 42\n': ['configs/base.yaml', '<Non-compressible placeholder 10>'], '18fa14a4d0ea4f4ea112268caa0afb3f__torch\nrequests\nmatplotlib\npyyaml\n': ['requirements.txt', '<Non-compressible placeholder 11>'], 'df1f747d31a2442380dce5c599dfcf67__# Transformer 模型从零实现项目\n\n## 1. 项目简介\n\n本项目旨在从零开始，使用 PyTorch 框架手工实现一个完整的 Transformer Encoder 模型。项目涵盖了模型核心组件的编码、训练流程的搭建，并在 WikiText-2 数据集上完成了语言建模任务的训练与评估。最终交付一个结构清晰、代码可复现的完整项目。\n\n## 2. 环境设置\n\n本项目依赖于 Python 3.x 和 PyTorch 框架。建议使用 Conda 或 venv 创建独立的虚拟环境。\n\n1.  **创建并激活虚拟环境 (以 Conda 为例):**\n    ```bash\n    conda create -n transformer_env python=3.9\n    conda activate transformer_env\n    ```\n\n2.  **安装项目依赖:**\n    ```bash\n    pip install -r requirements.txt\n    ```\n\n    `requirements.txt` 文件内容如下：\n    ```\n    torch\n    requests\n    matplotlib\n    pyyaml\n    ```\n\n## 3. 运行说明\n\n本项目提供了一个 `run.sh` 脚本，可以一键启动模型的训练和评估过程。为了确保结果的可复现性，训练过程中使用了固定的随机种子。\n\n**执行训练:**\n\n```bash\n./scripts/run.sh\n```\n\n或者直接运行 Python 脚本：\n\n```bash\npython src/train.py\n```\n\n**超参数配置:**\n\n所有超参数都定义在 `configs/base.yaml` 文件中，内容如下：\n\n```yaml\nmodel:\n  embed_dim: 128\n  n_heads: 4\n  ffn_dim: 512\n  n_layers: 2\n  dropout: 0.1\ntraining:\n  batch_size: 32\n  learning_rate: 3e-4\n  epochs: 5\n  bptt: 35\n  seed: 42\n```\n\n**数据集:**\n\n项目会自动下载并处理 WikiText-2 数据集。数据集文件将解压到项目根目录下的 `wikitext-2` 文件夹中。\n\n## 4. 硬件要求\n\n*   **GPU:** 建议使用 NVIDIA GPU 以加速训练过程。本项目在 NVIDIA GPU (例如，RTX 3060 或更高版本) 上进行了测试。\n*   **显存:** 至少 6GB 显存。\n*   **CPU:** 任意现代多核 CPU。\n*   **内存:** 至少 8GB RAM。\n\n## 5. 项目结构\n\n```\ntransformer-project/\n├── src/                  # 存放所有核心源代码 (.py 文件)\n│   ├── model.py          # Transformer模型定义\n│   ├── dataset.py        # 数据集处理与加载\n│   └── train.py          # 训练与评估脚本\n├── configs/              # 存放配置文件\n│   └── base.yaml         # 包含所有超参数的配置文件\n├── scripts/\n│   └── run.sh            # 一键运行训练的shell脚本\n├── results/              # 存放实验结果\n│   ├── training_curves.png # 训练/验证损失曲线图\n│   └── metrics.json      # 最终的评估指标\n├── requirements.txt      # 项目依赖库列表\n└── README.md             # 项目说明文档\n```\n': ['README.md', '<Non-compressible placeholder 12>'], '3c691938335447af8715123af06e73c2__File \'results/metrics.json\' read successfully! File content:\n```\n{\n    "final_validation_loss": 4.969256056540924,\n    "final_validation_perplexity": 143.91977935622225\n}\n```': ['results/metrics.json', '<Non-compressible placeholder 13>']}